# Progress Log (Development)

Purpose: Chronicle day-to-day development progress, decisions, fixes, blockers, and next actions for Flow-MBPO/PWM. Use this file to keep a running timeline of what changed and why. Keep entries concise, in English, and append-only (newest at top).

How to update:
- Add a dated heading for each work session.
- Bullet key changes, issues found, fixes applied, TODOs carried forward.
- Link to relevant PRs/commits, config files, scripts, and experiment entries in `docs/experiment_log.md`.
- Note any unresolved problems explicitly so they are not forgotten.

Template for each entry:
```
## YYYY-MM-DD â€“ Author
- Changes: ...
- Issues/risks: ...
- Follow-ups: ...
```

---

## 2026-01-04 17:30 â€“ Copilot (Storage Cleanup & Resubmission)
- **Issue**: All Phase 4/5 experiments failed due to disk quota exceeded (98.4% usage).
- **Actions**:
  1. Deleted all intermediate checkpoints (`model_*.pt` except best/last/final).
  2. Identified 50 completed runs with `Final reward` in logs.
  3. Deleted all weights from incomplete runs.
  4. Storage reduced from 98.4% to 51.5% (freed ~145GB).
- **Completed Experiments**: Phase 3 Baseline (18 runs) and Flow Policy (15 runs) fully completed.
- **Resubmitted**:
  - `4012433`: Full Flow Model (Array 0-8) - 9 jobs
  - `4012434`: Flow Hyperparameter Tuning (Array 0-17) - 18 jobs
- **Reorganized**: `experiment_log.md` completely rewritten for clarity.
- **Next**: Monitor new jobs, verify completion.

---

## 2026-01-04 01:00 â€“ Copilot (Phase 3 Complete)
- **Status**: All MT30 Baseline and Flow Policy experiments finished successfully.
- **Cleanup**: Processed resulting metrics and deleted all `.pt` weights to free **57.46 GB** (Total reclaimed: >100GB).
- **Findings**:
  - `reacher-easy`: Tie (Reward ~982).
  - `walker-stand`: Baseline better (958 vs 840).
  - `cheetah-run`: Both failed (112 vs 99). Previous high baseline score was an aggregation error.
- **Next**: Proceed to Full Flow Model (Phase 4).

### Phase 4: Full Flow & Debugging (Ongoing)
- **Date**: Jan 04, 2026
- **Status**: Launched experiments with improved infrastructure.
- **Improvements**:
  - **Best Model Tracking**: Implemented `model_best.pt` saving based on eval reward.
  - **Robust Resume**: Fixed `PWM.load` to restore iteration counts, optimizers, and `best_reward`.
  - **Detailed Logging**: New WandB project `MT30-Detailed` with per-epoch LR and grad stats.
- **Experiments**:
  - `4011967`: Full Flow (reacher, walker, cheetah).
  - `4011968`: Cheetah Debug (Horizon=30).

### Phase 5: Baseline Comparison & Tuning (Ongoing)
- **Date**: Jan 04, 2026
- **Status**: Launched massive parallel experiments.
- **Goal**: Establish fair "From Scratch" baseline and tune Flow params for performance.
- **Experiments**:
  - `4011987`: Baseline MLP (From Scratch) - 9 Jobs on `coc-gpu`. Running Healthy.
  - `4011988`: Flow Tuning (High Precision) - 18 Jobs on `ice-gpu`. Pending.
  - `4012027`: Full Flow (Resubmitted) - 9 Jobs on `ice-gpu`. Replaces `4011967` (Fixed `tqdm` hang).
  - `4012028`: Cheetah Debug (Resubmitted) - 3 Jobs on `ice-gpu`. Replaces `4011968` (Fixed `tqdm` hang).

---

- **Action**: Monitored training progress. Attempt 8/9 verified running.
- **Storage**: Scratch quota reached 84%. Ran `scripts/mt30/cleanup_weights.py` to delete `.pt` files from 10 completed runs (validating 10k epochs first).
- **Result**: Reclaimed **44.21 GB**.
- **Metrics**: Added partial evaluation table to `experiment_log.md`. Flow Policy showing strong results on `reacher-easy` (~980) and `walker-stand` (~890).

---

- **Incident**: Flow Policy jobs (Attempt 8, indices 3,5-8) failed immediately with `ERR! / 700W` on node `atl1-1-03-017-23-0`.
- **Diagnosis**: Hardware failure (Bad GPU) on specific node.
- **Recovery**: Resubmitted failed indices as **Attempt 9** (`4011740`) with `--exclude=atl1-1-03-017-23-0`.
- **Status**: Baseline jobs (`4011713`) queueing/running. Flow jobs (`4011714`) partially running, partially resubmitted.

---

- **Incident**: Attempt 7 (Array Jobs 4011522, 4011523) completed successfully, but default Hydra output directory naming (1-second precision) caused collisions. Simultaneous array tasks overwrote each other's logs in the same directory.
- **Outcome**: Only the last-writing seed per task survived. Partial data recovered (`cheetah-run` R~115, `walker-stand` R~901).
- **Fix**: Updated `submit_*.sh` to explicitly set `hydra.run.dir="outputs/mt30/${SLURM_JOB_ID}/${SLURM_ARRAY_TASK_ID}_s${SEED}"` ensuring unique paths.
- **Action**: Resubmitted full experiment batch as **Attempt 8** (Jobs `4011713`, `4011714`).

---

## 2026-01-03 19:10 â€“ Copilot (Attempt 7 - Confirmed Training)
- **Root Cause**: Attempts 5-6 failed with `TypeError: 'module' object is not callable` because adding `import time, random` at line 201 shadowed the earlier `from time import time` at line 17.
- **Fix**: Changed line 17 to `import time` and updated all `time()` calls to `time.time()` (lines 284, 294). Removed redundant import at line 201.
- **Resubmission**: Attempt 7 submitted as Baseline Job `4011522` and Flow Policy Job `4011523`.
- **Verification**: Training confirmed working with `[0/10000] AL:-1.402 VL:0.818`, `R: 56.40` on reacher-easy.

---

## 2026-01-03 18:45 â€“ Copilot (Attempt 4 Fixes & Stabilization)
- **WandB Metadata**: Moved `create_wandb_run` in `scripts/train_multitask.py` to after environment initialization. This ensures `action_dim` and `action_dims` are correctly populated in the WandB config.
- **Resource Stabilization**: Added a random start delay (0-60s) to `scripts/train_multitask.py` to mitigate concurrent resource spikes (VRAM/CPU) when launching large arrays on the same node. This addresses the CUDA OOM issues seen in the H200 jobs.
- **Job Recovery**: Resubmitted failed indices 5, 7, 8 for Flow Policy (Attempt 4) as array job `4011475`.
- **Checkpoint Resume**: Verified via code review that `PWM.load` with `resume_training=True` correctly restores optimizer states, training iterations, and step counts.
- **WandB Clarity**: Documented that `PWM_...` runs are zombied Attempt 3 artifacts and should be ignored. Valid runs now use `baseline_H100_...` or `flowpolicy_H200_...` prefixes.

---

## 2026-01-03 18:15 â€“ Copilot (MT30 Attempt 2 Submission)

### Fixes Applied
- **WandB Config Fix**: Resolved `KeyError: 'notes'` in `scripts/train_multitask.py` by using `wandb_cfg.get("notes", None)`. This prevents crashes when notes are missing from the configuration.
- **Improved Experiment Tracking**: Updated `docs/experiment_log.md` to track attempts separately and mark failed jobs with specific reasons.
- **Planned Verification**: Currently running `scripts/mt30/test_minimal.sh` (Job 4011376) to verify the fix and data loading on H100.

### Next Steps
- Submit full array of 18 jobs (Baseline vs Flow Policy) after minimal test passes.
- Monitor WandB for real-time training progress.

---

## 2026-01-03 04:37 â€“ Copilot (MT30 Full Training Submitted)

### Training Jobs Submitted
- **Baseline Array Job 4010895**: 9 jobs (3 tasks Ã— 3 seeds)
- **Flow Policy Array Job 4010896**: 9 jobs (3 tasks Ã— 3 seeds)
- **Total**: 18 training jobs

### Configuration
- **Tasks**: reacher-easy, walker-stand, cheetah-run (DMControl)
- **Seeds**: 42, 123, 456
- **GPU**: H100 (ice-gpu partition)
- **Memory**: 450GB
- **Time limit**: 16 hours
- **Epochs**: 10000

### Scripts Updated
- `submit_baseline.sh`: Updated for H100/ice-gpu, fixed task names (DMControl, not MetaWorld)
- `submit_flowpolicy.sh`: Same updates
- `validate_config.sh`: Created for quick validation testing

### Validation Test Passed
- **Job 4010894**: walker-stand seed 42, R=356.81, 2:34 runtime

---

## 2026-01-03 â€“ Copilot (MT30 Smoke Test SUCCESS)

### ðŸŽ‰ Smoke Test Completed Successfully!
- **Job ID:** 4010862
- **Task:** reacher-easy
- **Results:**
  - Final Reward: **1000.00**
  - Planning Reward: **993.00**
  - Runtime: 2:16
  - Data loaded: 24000 episodes from chunk_2.pt
  - Buffer: 12,024,000 capacity (1.59 GB on CUDA)

### Environment Fixes Applied
- **Conda activation fix:** Added `conda shell.bash hook` + `export PATH=$CONDA_PREFIX/bin:$PATH`
- **Missing packages installed:** dm_control, dm_env, lxml, pandas, hydra, omegaconf
- **Config fix:** Changed `pwm.utils.buffer` â†’ `flow_mbpo_pwm.utils.buffer`
- **PWM.save() fix:** Added `getattr()` defaults for missing attributes

### Code Changes
- `train_multitask.py`: Added `weights_only=False` for PyTorch 2.6 TensorDict compatibility
- `train_multitask.py`: Enhanced WandB logging with epoch, epoch_progress, learning_rate
- `pwm.py`: Fixed save() method to use getattr() for missing attributes
- `smoke_test.sh`: Fixed conda activation for proper environment detection

### Ready for Full Experiments
- âœ… Environment verified working on H100
- âœ… Data loading works (MT30: 24000 episodes)
- âœ… Training loop works
- âœ… Evaluation works (include planning)
- âœ… Model saving works
- âœ… WandB logging enhanced

---

## 2026-01-01 â€“ Copilot (Resume Support + MT30 Prep)
- **RESUME SUPPORT ADDED:**
  - Added `resume_from` parameter to `config_mt30.yaml` for checkpoint resume
  - Modified `train_multitask.py` to support full checkpoint loading with training state
  - Training loop now starts from `start_epoch` when resuming
  - Checkpoints saved every `eval_freq=200` epochs for recovery
- **DATA CLEANUP:**
  - Deleted MT80 data and checkpoint to save disk space
  - Keeping only MT30 for initial experiments
- **GIT COMMITS:** `46e2f56` (path fix), `2442639` (resume support)
- **READY FOR TRAINING:**
  - Checkpoints: `checkpoints/multitask/mt30_48M_4900000.pt` âœ…
  - Data: `/home/hice1/eliu354/scratch/Data/tdmpc2/mt30/` (4 chunks) âœ…

---

## 2025-12-29 â€“ Copilot (Multitask Branch Setup)
- **NEW BRANCH CREATED:** `dev/multitask` pushed to origin
- **NEW CONFIGS ADDED (baseline-aligned):**
  - `pwm_48M_mt_baseline.yaml`: 48M MLP WM + MLP Policy (baseline reference)
  - `pwm_48M_mt_flowpolicy.yaml`: 48M MLP WM + Flow ODE Policy
  - `pwm_48M_mt_fullflow.yaml`: 48M Flow WM + Flow ODE Policy
  - All configs use `wm_batch_size=256`, `wm_buffer_size=1_000_000` to match original PWM
- **NEW SLURM SCRIPTS:**
  - `scripts/mt30/submit_baseline.sh`: MT30 baseline submission (array jobs)
  - `scripts/mt30/submit_flowpolicy.sh`: MT30 Flow Policy submission
  - `scripts/mt30/submit_fullflow.sh`: MT30 Full Flow submission
  - `scripts/mt30/download_data.sh`: Download checkpoints and data instructions
- **TRAINING CODE VERIFICATION:**
  - `train_multitask.py` is identical to original PWM (only import path changed)
  - Conda env `flow-mbpo` verified at `/storage/ice1/2/9/eliu354/conda_envs/flow-mbpo`
- **DATA/CHECKPOINT PATHS:**
  - MT30 checkpoint: `checkpoints/mt30_48M_4900000.pt` (download from HuggingFace)
  - MT30 data: `/home/hice1/eliu354/scratch/Data/tdmpc2/mt30` (download from tdmpc2.com)
- **GIT COMMITS:** `a4b99b4`, `132b319`
- **NEXT STEPS:**
  1. Run `scripts/mt30/download_data.sh` to download checkpoints
  2. Download TD-MPC2 MT30 data manually from https://www.tdmpc2.com/dataset
  3. Submit baseline experiments first, then flowpolicy for comparison

---

## 2025-12-29 â€“ Copilot (Ant Experiments Setup)
- **ANT EXPERIMENTS LAUNCHED:**
  - Created `scripts/ant/` directory with smoke tests and full training scripts
  - Smoke tests: baseline, flowWM, flowpolicy, fullflow (jobs 3093139-3093142)
  - Full training: 18 jobs submitted (3093165-3093182)
  - Variants: Baseline, Flow WM (K=2,4,8), Flow Policy, Full Flow - 3 seeds each
  - WandB project: `flow-mbpo-single-task-ant`
- **FIXED STRONGREG CONFIG:**
  - Removed unsupported `wm_weight_decay` parameter
  - Changed to tighter gradient clipping for regularization
  - Resubmitted jobs 3093114-3093116
- **GIT COMMITS:** TBD
- **NEXT:** Monitor job status, verify smoke tests complete successfully

---

## 2025-12-26 â€“ Copilot (Baseline Restoration + Flow-MBPO Alignment)
- **BASELINE RESTORED TO ORIGINAL VALUES:**
  - `pwm_5M_baseline_final.yaml`: wm_batch_size=256, wm_buffer_size=1M, planning=False
  - `dflex_ant.yaml`: num_envs=128
  - Maximum epochs: 15,000 (matching original)
- **FLOW-MBPO CONFIGS ALIGNED WITH BASELINE:**
  - All flow configs (v1, v2, v3) now use same base params as baseline
  - Only flow-specific params differ: use_flow_dynamics, flow_integrator, flow_substeps
  - units=[512,512] aligned with baseline (was [510,510])
- **BASELINE IMMUTABILITY POLICY DOCUMENTED:**
  - Added to `master_plan.md` Section 2 Core Invariants
  - baselines/ added to .gitignore (local reference only)
- **GIT COMMIT:** `784b32f`
- **NEXT:** Push to GitHub, run comparison experiments

## 2025-12-26 â€“ Copilot (Codebase Restructure + Baseline Comparison)
- **CODEBASE RESTRUCTURED:**
  - Removed PWM git submodule completely (was at `PWM/`)
  - Copied src/, scripts/, setup.py, environment.yaml from thedannyliu/PWM dev/flow-dynamics branch to root
  - New structure: `src/pwm/` instead of `PWM/src/pwm/`
  - All 91 files committed in commit `6235671`
  - Import test passed: WorldModel, FlowWorldModel, PWM all accessible
- **BASELINE COMPARISON COMPLETED:**
  - Cloned imgeorgiev/PWM.git to `baselines/original_pwm/` for comparison
  - Created `docs/baseline_comparison.md` documenting all code differences
  - Key findings:
    * `world_model.py` is IDENTICAL to original
    * `pwm.py` has 437 lines changed (flow-matching support, monitoring)
    * `actor.py` adds FlowActor class for Phase 2 experiments
    * Config differences: `wm_batch_size` 256â†’1024, `num_envs` 128â†’256, `wm_buffer_size` 1Mâ†’2M
    * Evaluation uses real env reward instead of predicted reward
- **DOCUMENTATION UPDATED:**
  - Updated `master_plan.md` Section 10 (removed 'cd PWM' instruction)
  - Added `master_plan.md` Section 11 with new codebase structure
- **GIT COMMITS:**
  - `b8a8104`: Added original_pwm clone and baseline_comparison.md
  - `6235671`: Flatten codebase structure (91 files)
- **NEXT STEPS:**
  - Finalize documentation updates
  - Push to GitHub for collaboration

## 2025-12-08 â€“ Copilot (Session 6 CONTINUED: Job Submission Blitz + Documentation)
- **MASSIVE JOB SUBMISSION COMPLETED:**
  - Phase 1.5 (Ablations): Submitted 4 jobs (2590202-2590205) - H8/H16 Ã— reg base/strong sweeps
  - Phase 2 48M: Created and submitted 2 jobs (2590206-2590207) - Priority 3&4 at 48M scale
  - Total: **6 new jobs** submitted in parallel to maximize GPU utilization
  - All configured with fixed wandb naming: `{wm_type}WM_{policy_type}pol_{scale}_seed{seed}_H{H}_K{K}`
- **DOCUMENTATION OVERHAUL:**
  - Updated `experiment_log.md` with ALL completed job metrics (eval rewards, runtime, memory)
  - Added "Current Status" summary section with 5M/48M matrix completion tables
  - Documented key finding: **MLP WM + Flow policy = +41.7% improvement over baseline**
  - Added comprehensive "Wandb Run Mapping" section correlating Job IDs to wandb run IDs
  - Mapped runs by runtime correlation: 2581563â†’run-20251207_172413-5wyczum4, etc.
- **COMPLETED METRICS SUMMARY:**
  - 5M Matrix: Priority 1 (R=+16.53), Priority 3 (R=**+23.43** BEST), Priority 4 (R=+16.77), Priority 2 (running)
  - 48M: Baseline (R=+25.14), Flow WM (running)
  - All eval metrics extracted from job logs and documented
- **GIT COMMITS:**
  - Commit 1: Added 48M Priority 3&4 submission scripts (ce6f75a)
  - Commit 2: Updated experiment_log.md with metrics and status (c603ddd)
  - Commit 3: Added wandb run mapping documentation (ba1c8d2)
  - Pushed to origin/dev/flow-dynamics
- **CURRENT JOB STATUS (as of 12:36 EST):**
  - 2 running: Job 2588416 (Priority 2 5M @5h22m), Job 2581582 (Flow WM 48M @12h37m)
  - 6 pending: Jobs 2590202-2590207 (Phase 1.5 ablations + 48M Priority 3&4)
  - 4 completed: Jobs 2581563, 2581581, 2583678, 2583679 with eval results documented
- **NEXT STEPS:**
  - Monitor Phase 1.5 ablations when they start (H=8 performance investigation)
  - Check if 48M Flow WM (2581582) completes without OOM
  - Run additional eval episodes on completed checkpoints for statistical significance
  - Verify new jobs appear in correct wandb project (flow-mbpo-pwm)

---

## 2025-12-08 â€“ Copilot (Session 6: Wandb Logging Fix + OOM Analysis)
- **CRITICAL FIX: Wandb dashboard empty issue resolved:**
  - Root cause: `wandb.init()` called twice (train_dflex.py line 57 + PWM.train() via WandBLogger class)
  - This created competing wandb runs with conflicting step numbers
  - All training metrics rejected with "Steps must be monotonically increasing" errors
  - Only runtime was logged: `wandb-summary.json = {"_wandb":{"runtime":7}}`
  - Fix: Removed duplicate WandBLogger initialization in `src/pwm/algorithms/pwm.py`
  - Verification: Job 2588416 (resubmitted Flow WM 5M) shows NO monotonicity warnings
- **OOM Issue Analysis (for Nhi's question):**
  - Job 2581564 (5M Flow WM) killed at 393GB memory usage (384GB allocated)
  - **Location**: `pwm.algorithms.pwm.PWM.compute_wm_loss()` during world model training phase
  - **Cause**: Flow WM uses ~35% more memory than MLP WM due to:
    * ODE integration storing K=4 substeps Ã— batch_size Ã— latent_dim intermediate states
    * Velocity network forward passes accumulate gradients across substeps
    * Flow matching loss stores source, target, and interpolated states simultaneously
  - Memory peaked during backward pass through 8-iteration WM training loop at epoch 18193/20000
  - **Solution**: Increased memory allocation to 450GB in all scripts
  - Resubmitted as job 2588416 with fix
- **Training Progress - WM/Policy Matrix (per master_plan.md section 5.4):**
  - Priority 1 (MLP WM + MLP policy): âœ… COMPLETED - Job 2581563, R=-16.53, 2h26m, 59GB mem
  - Priority 2 (Flow WM + MLP policy): ðŸ”„ RESUBMITTED - Job 2588416 with wandb fix + 450GB
  - Priority 3 (MLP WM + Flow policy): âœ… COMPLETED - Job 2583678, R=-23.43, 1h58m, 39GB mem
  - Priority 4 (Flow WM + Flow policy): ðŸ”„ RUNNING - Job 2583679, 2h53m elapsed
- **48M Scale Experiments:**
  - 48M Baseline: âœ… COMPLETED - Job 2581581, R=-25.14, 3h44m, 63GB mem
  - 48M Flow WM: ðŸ”„ RUNNING - Job 2581582, 7h15m elapsed (may OOM with old 384GB allocation)
- **Key Insights:**
  - Flow policy (Priority 3) achieved R=-23.43 vs baseline R=-16.53 (+41.7% improvement!)
  - 48M baseline outperforms 5M baseline: R=-25.14 vs R=-16.53 (+52% improvement)
  - Flow WM requires 450GB vs MLP WM's 59GB (~7.6x memory) due to ODE integration overhead
- Git commits: c0fa16b (wandb logging fix)
- Next: Wait for Priority 2 & 4 to complete, then submit Phase 1.5 ablations

## 2025-12-07 â€“ Copilot (Session 5: Node-Specific CUDA Issue Resolution)
- **Diagnosed persistent CUDA device conflicts on specific node:**
  - Issue: All Phase 2 and Phase 1.5 jobs failing with "CUDA device busy" on node atl1-1-01-010-35-0
  - Root cause investigation:
    * Initially added `export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID` - didn't work
    * Then set `export CUDA_VISIBLE_DEVICES=0` which caused all jobs to compete for GPU 0
    * Removed manual CUDA_VISIBLE_DEVICES exports to let Slurm handle it
    * Added `--exclusive` flag for exclusive node access - still failed
    * Discovered all failures occurred on specific node atl1-1-01-010-35-0
  - Solution: Added nodelist constraint to use only verified working nodes
- **Resource allocation updates:**
  - Memory: 384GB â†’ 450GB (user request for larger allocation)
  - Time: 32h/40h â†’ 40h (standardized across all jobs)
- **Successfully queued Phase 2 jobs:**
  - Jobs 2583678-2583679: Phase 2 MLP WM + Flow policy, Flow WM + Flow policy â†’ PENDING (Resources)
  - Constrained to working nodes: atl1-1-03-007-29-0, atl1-1-03-007-31-0, atl1-1-03-004-29-0, atl1-1-03-004-31-0, atl1-1-01-004-31-0, atl1-1-01-004-33-0
- **Phase 1 training progress:**
  - Job 2581563 (5M baseline): 84% complete (12630/15000 epochs), ETA ~22 min
  - Job 2581564 (5M Flow WM): 31% complete (6251/20000 epochs), ETA ~4.5h
  - Jobs 2581581-2581582 (48M): Running ~2h elapsed
- **Complete WM/Policy matrix status:**
  - Priority 1: MLP WM + MLP policy (Job 2581563) - 84% complete âœ“
  - Priority 2: Flow WM + MLP policy (Job 2581564) - 31% complete âœ“
  - Priority 3: MLP WM + Flow policy (Job 2583678) - Queued, waiting for resources
  - Priority 4: Flow WM + Flow policy (Job 2583679) - Queued, waiting for resources
- Git commits: 2236ec8 (remove CUDA_VISIBLE_DEVICES), e9aaac6 (add --exclusive), 4b97cad (add nodelist constraint)
- Next: Monitor Phase 1 completion, Phase 2 jobs will start automatically when resources available, then submit Phase 1.5 ablations

## 2025-12-07 â€“ Copilot (Session 4: Final Fixes and Full Deployment)
- **Fixed critical CUDA device conflicts:**
  - Issue: Multiple jobs failing with "CUDA device busy or unavailable"
  - Root cause: dflex_ant config uses 256 parallel envs (too much for L40s 48GB)
  - Solution 1: Switch to dflex_ant_l40s config (128 envs)
  - Solution 2: Add `export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID` to isolate GPU access
- **Successfully deployed all 10 jobs (Phase 1/1.5/2):**
  - Jobs 2581563-2581564: Phase 1 5M baseline + Flow WM â†’ RUNNING with wandb
  - Jobs 2581581-2581582: Phase 1 48M baseline + Flow WM â†’ Queued
  - Jobs 2581583-2581586: Phase 1.5 H8/H16 reg sweeps â†’ Queued
  - Jobs 2581587-2581588: Phase 2 Flow policy variants â†’ Queued
- **Wandb tracking confirmed working:**
  - Project: `flow-mbpo-pwm` (danny010324)
  - Baseline 5M job training at 1% (122/15000 epochs)
  - Links visible in logs: https://wandb.ai/danny010324/flow-mbpo-pwm
- **Phase 2 WM/Policy matrix coverage:**
  - MLP WM + MLP policy: Phase 1 baseline âœ“
  - Flow WM + MLP policy: Phase 1 Flow WM âœ“
  - MLP WM + Flow policy: Phase 2 job 2581588 âœ“
  - Flow WM + Flow policy: Phase 2 job 2581587 âœ“
- Git commit 4957af9: CUDA fixes and env config.
- Next: Monitor jobs (ETA 8-12h for 5M, 24-30h for 48M), extract metrics, analyze Phase 1.5 results for canonical config.

## 2025-12-07 â€“ Copilot (Session 3: Successful Training Launch)
- **Fixed critical Hydra parsing errors in wandb config overrides:**
  - Issue 1: Single `+` vs double `++` for existing config keys (wandb.notes) â†’ fixed with sed
  - Issue 2: Equals signs in wandb field values (K=4, H=16, L2=3e-4) interpreted as nested config â†’ removed all `=` from wandb strings
  - Issue 3: Parentheses in wandb.notes causing parse errors â†’ removed all parentheses
- **Successfully submitted all Phase 1/1.5/2 training jobs (10 total):**
  - Phase 1: Jobs 2572352-2572355 (5M+48M baseline+Flow WM) â†’ all TRAINING
  - Phase 1.5: Jobs 2572356-2572359 (5M Flow WM H8/H16 reg sweeps) â†’ 1 running, 3 queued
  - Phase 2: Jobs 2572360-2572361 (5M Flow policy variants) â†’ both queued
- **Verified training startup:**
  - Baseline (job 2572354): WorldModel 1.52M params, self-check passed, training started
  - Flow WM (job 2572355): FlowWorldModel 1.51M params, Heun K=4, self-check passed, training started
- All job logs confirm models initialize correctly, no NaN/config errors.
- Git commit 74e537f: Hydra-safe wandb fields fix.
- Next: Monitor jobs over hours/days, collect final metrics, update experiment_log.md with results.

## 2025-12-07 â€“ Copilot (Session 2: Initial Job Submissions - FAILED)
- Fixed bash strict mode (`set -euo pipefail`) conflicts with cluster bashrc; removed from all scripts.
- Submitted smoke test (job 2571819) â†’ succeeded, models initializing correctly.
- **First job submission attempt (jobs 2571823-2571839):**
  - All 10 jobs failed immediately due to Hydra ConfigCompositionException
  - Root cause: Used `+wandb.project` but wandb already exists in config.yaml (should use `++` for override)
- Applied sed fix: `+wandb.*` â†’ `++wandb.*` across all scripts.
- **Second submission attempt (jobs 2571866-2571882):**
  - Some jobs still failed with "mismatched input '(' expecting <EOF>" and "mismatched input '=' expecting <EOF>"
  - Root cause: Parentheses and equals signs in wandb field values confuse Hydra parser
- Issues documented; fixes applied in Session 3.

## 2025-12-07 â€“ Copilot (Session 1: Setup)
- Added `wm_weight_decay` option to PWM optimizer to support Phase 1.5 regularization sweeps.
- Added structured Slurm scripts under `scripts/phase1`, `scripts/phase1p5`, and `scripts/phase2` for baseline vs Flow WM (5M/48M), Flow WM reg grid (Hâˆˆ{8,16}, base/strong L2), and Flow policy runs.
- Translated remaining Chinese strings to English in `scripts/train_dflex.py` and `scripts/submit_all_verified.sh` to keep comments/logs consistent.
- FlowActor smoke test on CPU passed (action shape [4,3], logstd shape [3]).

## 2025-12-07 â€“ Copilot
- Added experiment operations/naming/logging guidance to `docs/master_plan.md` (section 10).
- Initialized progress log and experiment log templates to standardize record-keeping.
- Next: start scheduling Phase 1/1.5/2 runs on L40S (baseline vs Flow) using the documented Slurm headers and update `docs/experiment_log.md` per submission.
