# 48M Flow Policy config: MLP World Model + Flow ODE Policy
# For multitask experiments (MT30/MT80) comparing Flow Policy vs MLP baseline
# Matches pwm_48M.yaml baseline hyperparameters, only swapping policy
_target_: flow_mbpo_pwm.algorithms.pwm.PWM
_recursive_: False

actor_config:
  _target_: flow_mbpo_pwm.models.flow_actor.ActorFlowODE
  units: [400, 200, 100]
  activation_class: nn.Mish
  init_gain: 1.0
  flow_substeps: 2           # Policy substeps (keep low for speed)
  flow_integrator: heun

critic_config:
  _target_: flow_mbpo_pwm.models.critic.CriticMLP
  units: [400, 200]
  activation_class: nn.Mish

world_model_config:
  _target_: flow_mbpo_pwm.models.world_model.WorldModel
  units: [1792, 1792]
  encoder_units: [1792, 1792, 1792]
  num_bins: 101
  vmin: -10.0
  vmax: 10.0
  task_dim: 0  # Will be overridden at runtime for multitask
  multitask: False  # Will be overridden at runtime
  action_dims: null
  tasks: null
  encoder:
    last_layer: normedlinear
    last_layer_kwargs:
      act:
        _target_: flow_mbpo_pwm.models.mlp.SimNorm
        simnorm_dim: 8
  dynamics:
    last_layer: normedlinear
    last_layer_kwargs:
      act:
        _target_: flow_mbpo_pwm.models.mlp.SimNorm
        simnorm_dim: 8
  reward:
    last_layer: linear
    last_layer_kwargs: {}

num_critics: 3
latent_dim: 768

actor_lr: 5e-4
critic_lr: 5e-4
model_lr: 3e-4
lr_schedule: linear

obs_rms: False
rew_rms: False
ret_rms: True

critic_iterations: 8
critic_batches: 4
critic_method: td-lambda
lam: 0.95
gamma: 0.99

max_epochs: 15_000
horizon: ${horizon}

actor_grad_norm: 1.0
critic_grad_norm: 100.0

wm_batch_size: 768
wm_iterations: 8
wm_grad_norm: 20.0
wm_buffer_size: 2_000_000

detach: True
save_interval: 2500
device: ${general.device}

# Note: This is MLP WM + Flow Policy for policy-only comparison
# World model uses baseline MLP dynamics, policy uses Flow ODE
use_flow_dynamics: false
