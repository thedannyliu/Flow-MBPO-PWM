_target_: flow_mbpo_pwm.algorithms.pwm.PWM
_recursive_: False
actor_config:
  _target_: flow_mbpo_pwm.models.actor.ActorStochasticMLP
  units: [1024, 512, 256]
  activation_class: nn.Mish
  init_gain: 1.0
  init_logstd: -1.0
  min_logstd: -1.427
critic_config:
  _target_: flow_mbpo_pwm.models.critic.CriticMLP
  units: [1024, 512]
  activation_class: nn.Mish
world_model_config:
  _target_: flow_mbpo_pwm.models.world_model.ProbabilisticWorldModel
  units: [1792, 1792]
  encoder_units: [1024, 1024, 1024]
  num_bins: 101
  vmin: -10.0
  vmax: 10.0
  task_dim: 0
  multitask: False
  action_dims: null
  tasks: null
  encoder:
    last_layer: normedlinear
    last_layer_kwargs:
      act:
        _target_: flow_mbpo_pwm.models.mlp.SimNorm
        simnorm_dim: 8
  dynamics:
    last_layer: normedlinear
    last_layer_kwargs:
      act:
        _target_: flow_mbpo_pwm.models.mlp.SimNorm
        simnorm_dim: 8
  reward:
    last_layer: linear
    last_layer_kwargs: {}
num_critics: 3
latent_dim: 768
actor_lr: 3e-4  # IMPROVED: 從 5e-4 降低 (大模型更穩定)
critic_lr: 3e-4  # IMPROVED: 從 5e-4 降低
model_lr: 1e-4  # IMPROVED: 從 3e-4 降低 (更關鍵)
lr_schedule: constant  # IMPROVED: 從 linear 改為 constant
obs_rms: False
rew_rms: False
ret_rms: True
critic_iterations: 8
critic_batches: 4
critic_method: td-lambda
lam: 0.95
gamma: 0.99
max_epochs: 30_000  # IMPROVED: 從 15000 增加到 30000 (大模型需更長訓練)
horizon: ${horizon}
actor_grad_norm: 1.0
critic_grad_norm: 100.0
wm_batch_size: 2048  # IMPROVED: 從 768 增加到 2048 (關鍵改進)
wm_iterations: 8
wm_grad_norm: 10.0  # IMPROVED: 從 20.0 降低 (更嚴格的梯度裁剪)
wm_buffer_size: 2_000_000
detach: True
save_interval: 5000  # IMPROVED: 從 2500 增加 (減少 I/O)
device: ${general.device}
